\documentclass{article}
\usepackage[left=3cm, right=3cm, top=3cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{graphicx}
\begin{document}

\noindent{\large Note 7}\\
(RSA)
\begin{itemize}
	\item Def: Public key is $(N,e)$ where $N = pq$ s.t. $p,q$ are large primes. $e$ (typically small $\mathbb{Z^+}$) such that $e,(p-1)(q-1)$ coprime. Private key is $d$ s.t. $d\equiv e^{-1}\pmod{(p-1)(q-1)}$. Original message $x$ encrypted into $y\equiv E(x) = x^e\pmod{{\color{red} N}}$, decryption $D(y) = y^d\equiv x^{ed}\equiv x\pmod{N}$.
	(Extra: $de-1\leq e(p-1)(q-1)$)
	\item Th 7.1: $D(E(x)) = x \pmod{N}$ $\forall$ $x \in \{0,1,...,N-1\}$.
	\item Th 7.2 (FLT): For any prime $p$ and $a\in\{1,2,...,p-1\}$, then $a^{p-1}\equiv 1 \pmod{p}$ (Extra: for any $x$, $x^y = x^{y\pmod{p-1}}\pmod{p}$, $x^p\equiv x\pmod{p}$).
	\item Th 7.3 (Prime Number Theorem): Let $\pi(n)$ denote \# of primes that are $\leq n$. Then $\forall n\geq17$, we have $\pi(n)\geq \frac{n}{\ln n}$ (in fact $\lim_{n\to\infty}\frac{\pi(n)}{n/\ln n} = 1$).
	\item HW: $a^{p(p-1)}\equiv1\pmod{p^2}$ where $p$ is prime and $a,p$ are coprime
	\item HW: If $a\equiv b\pmod{m_1}, a\equiv b\pmod{m_2}$ and gcd$(m_1, m_2) = 1$, then $a\equiv b\pmod{m_1m_2}.$
	\item HW (composite FLT): If $n = p_1p_2\cdot\cdot\cdot p_k$ where $p_i$ are distinct primes and $(p_i - 1)\mid(n - 1)\ \forall i$, then $a^{n-1}\equiv1\pmod{n}\ \forall a\in\{i\ |\ 1\leq i\leq n\ \land$ gcd$(n,i) = 1\}.$
	\item Extra: If $ax+bm = d$, gcd$(x,m) = d, xu\equiv v\pmod{m},$ then it has a solution $\iff d\mid v$; if so, one such solution is $u\equiv\frac{va}{d}\pmod{\frac{m}{d}}$, (in essence, $a = (\frac{x}{d})^{-1}\pmod{\frac{m}{d}}$,) and there are exactly {\large$d$}-many solutions (of the form $u = \frac{va}{d} + i\cdot\frac{m}{d}\pmod{m}).$
	\item Extra: $(p-1)!\equiv(p-1)\pmod{p}$: Proof by the fact that $2,...,p-2$ will pair up with their own inverse, and only ones that map back to themselves are $1, -1$.
	\item Extra: The existence of $a^{-1}$ allows us to conclude that $ar\pmod{n}$ to $r\pmod{n}$ is a bijection.
	\item Extra: If there are $k$ numbers that are relatively prime to $N$ in $\{0,\dots,N-1\}$, then $a^k\equiv1\pmod{N}$ if gcd$(a,N)=1$ (Proof $\sim$FLT).
\end{itemize}

\noindent{\large Note 8}\\
(Polynomials)
\begin{itemize}
	\item Fundamental properties (1) A non-zero polynomial of deg $d$ has at most $d$ roots; (2): $d+1$ distinct pairs $(x_1,y_1),\dots,(x_{d+1},y_{d+1})$ uniquely decide $p(x)$ of degree (at most) $d$ such that $p(x_i)=y_i$ for $1\leq i\leq d+1$.
	\item Lagrange interpolation (Construct the unique $p(x)$ of deg $\leq d$ from $d+1$ pairs)
	\begin{enumerate}
		\item Def: Let $\Delta_i(x)$ be the degree $d$ polynomial that goes through these $d+1$ points, construct $\Delta_i(x) = \frac{\prod_{j\neq i} (x-x_j)}{\prod_{j\neq i} (x_i-x_j)}$ for $i\in\{1,\dots,d+1\}$ with the given pairs.
		\item Construct $p(x) = \sum\limits_{i=1}^{d+1} y_i\Delta_i(x)$
	\end{enumerate}
	%\item Polynomial division: $p(x) = q'(x) q(x) + r(x)$ where deg$(r(x)) <$ deg$(q(x))\leq$ deg$(p(x)) = d$
	\item Finite field: $GF(p)$ or $F(p)$ when we work with numbers modulo a prime $p$
	%\item In general, we can specify a degree $d$ polynomial $p(x)$ by specifying its values at $d+1$ points, say $x=0,1,\dots,d$. These $d+1$ values, $(y_0,y_1,\dots,y_d)$, are called the \textit{value representation} of $p(x)$. The \textit{coefficient representation} can be converted to the value representation by evaluating the polynomial at $0,1,...,d$.
	\item Note that the reason that we can count the number of polynomials in this setting is because we are working over a finite field.

\begin{figure} [h!]
\begin{center}
	\includegraphics[width=0.5\linewidth]{/Users/Alan/Desktop/70_polynomial}
	\label{fig}
\end{center}
\end{figure}
	\item Secret Sharing utilizes Lagrange interpolation and works under $GF(p)$. {\color{red} N.B.}, two people have no more information about the secret than one.
	\item HW: Min/Max number of roots for non-zero real polynomials $f$ and $g$
	\begin{itemize}
		\item $f+g$: {\color{red} min} = 0 if sum is even degree, 1 if odd degree; {\color{red} max} = max(deg$(f)$, deg$(g)$) if sum is non-zero, and infinity if $f=-g$
		\item $f\cdot g$: {\color{red} min} = 0; {\color{red} max} = deg$(f)$ + deg$(g)$
		\item $f/g$ (assume polynomial): {\color{red} min} = 0; {\color{red} max} = deg$(f)$ - deg$(g)$
	\end{itemize}
	\item HW: Consider $f = x^{p-1}-1, g=x$ be polynomials over $GF(p)$. Both are non-zero polynomials, but their product have a zero on all points in $GF(p)$.
	\item HW: If $f$ is under $GF(p)$ such that deg$(f)\geq p$, then there exists a polynomial $h$ with deg$(h) < p$ such that $f(x) = h(x)$ for all $x \in \{0, 1, \dots, p-1\}$.
	\item HW: There are ${\color{red} (p-1)}\cdot p^{d-1}$ many $f$ of degree \textit{exactly} $d < p$ over $GF(p)$ such that $f(0) = a$ for some fixed $a \in\{0,1,\dots,p-1\}$
	\item Extra: There are $p^p$ polynomials in $GF(p)$ for some prime $p$.
	\item Extra: $(x-1)(x-2)\cdots(x-p+1)\equiv x^{p-1}-1 \pmod{p}$
\end{itemize}

\noindent {\large Note 9}\\
(Error Correction)
\begin{itemize}
	\item In mod, $\frac{a}{x} \equiv ax^{-1}\pmod{p}$
	\item If we want to send $x$ packets, work under $GF(p)$ where {\color{red} $p>x$} and $p$ is prime.
	\item Consider sharing an $n$-bit secret, where the secret is encoded as $P(0)$ for a polynomial of degree $k$ modulo $p$ where $s$ shares will be handed out, then $p\geq\text{max}(2^n, k+1, s+1)$.
	\item Erasure Errors: At least $(n+k)$ packets for $k$ erasure errors. We can uniquely reconstruct $P(x)$ from its values at \textit{any} $n$ distinct points via Lagrange interpolation, since it has degree $n-1$.
	\item General Errors: At least $(n+2k)$ packets for $k$ general/corruption errors (Extra: The distance between any two codewords must be $2k+1$).
	\begin{itemize}
		\item Error-locator polynomial (deg $k$): $E(x) = (x-e_1)(x-e_2)\dots(x-e_k)$, so $k$ of $k+1$ coefficients unknown.
		\item {\color{red} $P(i)E(i) = r_iE(i)$ for $1\leq i\leq n+2k$}
		\item Define $Q(x) := P(x)E(x)\ (= r_xE(x))$, which is a polynomial of degree $(n+k-1)$, and is therefore described by $n+k$ coefficients $a_0,a_1,\dots,a_{n+k-1}$
		\item We have $(n+2k)$ packets (linear equations), one for each value of $i$, and $(n+2k)$ unknowns. Solve for $E(x)$ and $Q(x)$. Compute $\frac{Q(x)}{E(x)}$ to obtain $P(x)$.
	\end{itemize}
	\item Error Detection (HW): At least $(n+k)$ packets to detect any errors, for maximum of $k$ errors.
	\item Extra: To send $n$-bit message through a channel that (1) loses fraction $f$ of packets, need $\frac{n}{1-f}$ packets; (2) corrupts fraction $f$ of packets, need $\frac{n}{1-2f}$ packets.
\end{itemize}

\noindent {\large Note 10} \\
(Infinity and Countability)
\begin{itemize}
	\item Def: A set $S$ is countable if there is a bijection between $S$ and $\mathbb{N}$ or some subset of $\mathbb{N}$. (Extra: if there's an injection from $S$ to $\mathbb{N}$, then latter part holds.)
	\item The Cantor-Bernstein theorem (?): $|B|\leq|A|\leq|B| \iff A,B$ have the same cardinality. (one-to-one $+$ onto (reversed 1-to-1) $\rightarrow$ bijection)
	\item Notes: The set of all (finite-degree) polynomials with $\mathbb{N}$ coefficients, $N(x)$, is countable.
	\item Th: The real interval $\mathbb{R}[0, 1]$ (and hence also $\mathbb{R}$) is uncountable. (Proof by Diagonalization)
	\item Th: $|\mathscr{P}(\mathbb{N})| > |\mathbb{N}|$ (Extra: The power set of any infinite set is uncountable).
	\item Disc: The countable union of countable sets is countable.
	\item Disc: The subset of any countable set is countable, so the intersection of (a countable or uncountable number of) countable sets is countable.
	\item Disc: The functions from $\mathbb{N}$ to $\mathbb{N}$ is uncountable. (Diag.)
	\item Disc: The total number of programs is countably infinite (each is a string of characters), so halting programs $\subseteq$ all programs is countable. Since there are $\infty$ halting programs (e.g. for each $i\in\mathbb{N}$ the program that prints $i$), so the total number of halting programs is countably infinite. (Not every function from $\mathbb{N}$ to $\mathbb{N}$ can be written as a program.)
	\item Disc: Given $A$ is countable, non-empty. $\forall i\in A$, $S_i$ is uncountable; $B$ is uncountable. $\forall i\in B$, $Q_i$ is countable.
	\begin{itemize}
		\item $A\cap B$ countable; $A\cup B$ uncountable
		\item $\cup_{i\in A} S_i$ uncountable; $\cap_{i\in A} S_i$ both could happen (disjoint vs. identical)
		\item $\cup_{i\in B} Q_i$ both could happen (identical vs. $B=\mathbb{R}, Q_i = \{i\}$); $\cap_{i\in B} Q_i$ countable
	\end{itemize}
	\item Disc: Not possible to determine if we ever execute a specific line (depends on the logic of the program), but the number of steps/instructions can be counted.
	\item Extra: Finite graphs $(V,E)$ is countably infinite (defined isomorphically).
\end{itemize}

\noindent {\large Note 11} \\
(Self-Reference and Computability)
\begin{itemize}
	\item Th: The Halting Problem is uncomputable; i.e., \textit{TestHalt} that can determine, on all inputs $(P, x)$, whether the program $P$ halts on input $x$ DNE.
	\item EastHalt is uncomputable (testing whether $P$ halts on input 0). (Proof by Reduction)
	\item Disc: We can determine whether a program halts/outputs sth in $k$ steps, but not before $k^{th}$ line.
	\item HW
	\begin{itemize}
		\item $f(x)=p\pmod{x}$, where $p>2$ is a prime, then $f:\{\frac{p+1}{2},\dots,p\} \rightarrow \{0,\dots,\frac{p-1}{2}\}$ is a bijection.
		\item Countable: $A\times B$, where $A$ and $B$ are both countable.
		\item Countable: $\cup_{i\in A} B_i$ where $A, B_i$ are all countable
		\item Uncountable: The set of all functions $f:\mathbb{N}\rightarrow\mathbb{N}$ such that $f$ is non-decreasing, i.e. $f(x)\leq f(y)$ whenever $x\leq y$. (Proof by Diag.)
		\item Countable: The set of all functions $f:\mathbb{N}\rightarrow\mathbb{N}$ such that $f$ is non-increasing, i.e. $f(x)\geq f(y)$ whenever $x\geq y$.
		\item Uncountable: The set of all bijective functions from $\mathbb{N}$ to $\mathbb{N}$.
	\end{itemize}
	\item Extra: We can determine whether a program contains a loop, given {\color{red} finite memory or limited time/steps} (since we can list all possible $k$ states and run the program $k+1$ steps). We can't check an arbitrary program as a whole w/o any restriction.
	\item Extra: There is a computer program that prints all programs and the inputs where they halt.
\end{itemize}

\noindent {\large Note 12} \\
(Counting)
\begin{itemize}
	\item First Rule of Counting: $n_1n_2\cdots n_k$ (order matters)
	\item Second Rule of Counting: $\binom{n}{k}$ (order doesn't matter)
\begin{center}
	\begin{tabular}{ |c|c|c| } 
 	\hline
 	 & Replacement & No Replacement \\
 	\hline
 	Order & $n^k$ & $\frac{n!}{(n-k)!}$ \\
 	\hline
 	No Order & $\binom{n+k-1}{n-1} = \binom{n+k-1}{k}$ & ? $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ \\ 
 	\hline
	\end{tabular}
\end{center}
	\item HW: If there are exactly 20 students currently enrolled in a class, then there are $19\cdot17\cdots3\cdot1 = \frac{20!}{10!\cdot2^{10}} = \frac{\binom{20}{2}\binom{18}{2}\cdots\binom{2}{2}} {10!}$ different ways to pair up the 20 students.
	\item Disc: There are $\sum\limits_{k=3}^n \frac{n!}{(n-k)!\cdot2k}$ distinct cycles in a complete graph with n vertices. Assuming no duplicated edges, two cycles are the same by rotation or inversion.
\end{itemize}

\noindent {\large Note 13} \\
(Discrete Probability)
\begin{itemize}
	\item Def: The outcome of a random experiment is called a sample point, and the sample space $\Omega$ is the set of all possible outcomes of the experiment.
	\item Def: A \textbf{probability space} is a \textbf{sample space} $\Omega$ with $\mathbb{P}[\omega]$ (Pr[$\omega$]) for each sample point $\omega$, such that
	\begin{itemize}
		\item (Non-negativity): $0\leq\mathbb{P}[\omega]\leq1$ for all $\omega\in\Omega$
		\item (Total one): $\sum\limits_{\omega\in\Omega} \mathbb{P}[\omega] = 1$, i.e., the sum of the probabilities over all outcomes is 1.
	\end{itemize}
	\item If the coin has $\mathbb{P}(H) = p$. Consider any sequence of $n$ flips with exactly $r$ $H$'s, then $\mathbb{P}$[this sequence] $= p^r (1-p)^{n-r}$. Now consider the event $C$ that we get exactly $r$ $H$’s when we flip the coin $n$ times, so $\mathbb{P}[C] = \binom{n}{r} p^r (1-p)^{n-r}$
	\item If we throw $m$ labeled balls into $n$ labeled bins, we have a sample space of size $n^m$ (generalized).
	\item Throw $m$ (identical or distinct) balls into $n$ distinct bins with uniform probability, then $\mathbb{P}[$the first bin has \textbf{exactly} $k$ balls$]=$ $\binom{m}{k}\cdot(\frac{1}{n})^k\cdot (1-\frac{1}{n})^{m-k}$
\end{itemize}

\noindent {\large Note 14} \\
(Conditional Probability)
\begin{itemize}
	\item Def 14.1 (Conditional Probability): For events $A,B \subseteq\Omega$ in the same Pr space such that $\mathbb{P}[B]>0$, the conditional Pr of $A\ given\ B$ is $\mathbb{P}[A|B] = \sum\limits_{\omega\in A\cap B} \mathbb{P}[\omega|B] = \sum\limits_{\omega\in A\cap B} \frac{\mathbb{P}[\omega]}{\mathbb{P}[B]} = \frac{\mathbb{P}[A\cap B]}{\mathbb{P}[B]}$
	\item $\mathbb{P}[A\cap B] = \mathbb{P}[A|B]\cdot\mathbb{P}[B] = \mathbb{P}[B|A]\cdot\mathbb{P}[A]\longrightarrow\mathbb{P}[A|B] = \frac{\mathbb{P}[B|A]\mathbb{P}[A]}{\mathbb{P}[B]}$ Bayes' Rule
	\item $\mathbb{P}[B] = \mathbb{P}[A\cap B] + \mathbb{P}[\overline{A}\cap B] = \mathbb{P}[B|A]\mathbb{P}[A] + \mathbb{P}[B|\overline{A}]\mathbb{P}[\overline{A}] = \mathbb{P}[B|A]\mathbb{P}[A] + \mathbb{P}[B|\overline{A}](1-\mathbb{P}[A])$ Total Probability Rule
	\item Thus, combining the two equations above gives: {\color{red} $\mathbb{P}[A|B] = \frac{\mathbb{P}[B|A]\mathbb{P}[A]}{\mathbb{P}[B]} = \frac{\mathbb{P}[B|A]\mathbb{P}[A]}{\mathbb{P}[B|A]\mathbb{P}[A] + \mathbb{P}[B|\overline{A}](1-\mathbb{P}[A])}$}
	\item Def 14.2 (Partition of an event): We say that an event $A$ is partitioned into $n$ events $A_1,\dots,A_n$ if
	\begin{enumerate}
		\item $A = A_1\cup A_2\cup \cdots \cup A_n$, and
		\item $A_i\cap A_j = \emptyset$ for all $i\neq j$ (mutually exclusive)
	\end{enumerate}
	\item Let $A_1,\dots,A_n$ be a partition of the sample space $\Omega$. Then, the Total Probability Rule for any event $B$ is $\mathbb{P}[B] = \sum\limits_{i=1}^n \mathbb{P}[B\cap A_i] = \sum\limits_{i=1}^n \mathbb{P}[B|A_i]\mathbb{P}[A_i]$, and the Bayes' Rule (assuming $\mathbb{P}[B]\neq0$) is $\mathbb{P}[A_i|B] = \frac{\mathbb{P}[B|A_i]\mathbb{P}[A_i]}{\mathbb{P}[B]} = \frac{\mathbb{P}[B|A_i]\mathbb{P}[A_i]}{\sum\limits_{j=1}^n \mathbb{P}[B|A_j]\mathbb{P}[A_j]}$
	\item Def 14.3 (Independence): Two events $A,B\in\Omega$ are independent $\iff\mathbb{P}[A\cap B] = \mathbb{P}[A]\times \mathbb{P}[B]$. For events $A,B$ such that $\mathbb{P}[B] > 0$, the condition $\mathbb{P}[A|B] = \mathbb{P}[A]\equiv$ independence.
	\item If $X,Y$ are independent, then $\mathbb{E}[X]\mathbb{E}[Y] = \mathbb{E}[XY]$, but the converse isn't always true.
	\item Def 14.4/5 (Mutual independence): Events $A_1,\dots,A_n$ are said to be mutually independent if (two equivalent definitions):
	\begin{itemize}
		\item for {\color{red} EVERY} subset $I\subseteq \{1,\dots,n\}$ with size $|I|\geq2$, we have $\mathbb{P}[\cap_{i_\in I} A_i] = \prod\limits_{i\in I} \mathbb{P}[A_i]$
		\item for all $B_i\in\{A_i, \overline{A_i}\}, i=1,\dots,n$, we have $\mathbb{P}[B_1\cap\cdots\cap B_n] = \prod\limits_{i=1}^n \mathbb{P}[B_i]$
	\end{itemize}
	\item The independence of every pair of events (so-called pairwise independence) does not necessarily imply mutual independence.
	\item Theorem 14.1 (Product Rule): $\mathbb{P}[A\cap B] = \mathbb{P}[A]\cdot\mathbb{P}[B|A]$. More generally, for any events $A_1,\dots, A_n$, we have
	$\mathbb{P}[\cap_{i=1}^n A_i] = \mathbb{P}[A_1]\cdot\mathbb{P}[A_2|A_1]\cdots\mathbb{P}[A_3|A_1\cap A_2]\cdots\mathbb{P}[A_n|\cap_{i=1}^{n-1} A_i]$ (Proof by Induction)
	\item Theorem 14.2 (Inclusion-Exclusion): Let $A_1,\dots,A_n$ be events in some probability space, where $n\geq2$. Then, we have $\mathbb{P}[\cup_{i=1}^n A_i] = \sum\limits_{i=1}^n \mathbb{P}[A_i] - \sum\limits_{i<j}\mathbb{P}[A_i\cap A_j] + \sum\limits_{i<j<k} \mathbb{P}[A_i\cap A_j\cap A_k] -\cdots +(-1)^{n-1}\mathbb{P}[A_1\cap A_2\cap\cdots\cap A_n]$ (Proof by Induction)
	\item (Mutually exclusive events) If the events $A_1,\dots,A_n$ are mutually exclusive, then $\mathbb{P}[\cup_{i=1}^n A_i] = \sum\limits_{i=1}^{n} \mathbb{P}[A_i]$
	\item (Union bound) Upper bound always due to overestimating: $\mathbb{P}[\cup_{i=1}^n A_i] \leq \sum\limits_{i=1}^{n} \mathbb{P}[A_i]$
\end{itemize}

\noindent {\large Note 15} \\
(Random Variables, Expectation)
\begin{itemize}
	\item Def 15.1 (Random Variable): A random variable $X$ on a sample space $\Omega$ is a function $X: \Omega\rightarrow\mathbb{R}$ that assigns to each sample point $\omega\in\Omega$ a real number $X(\omega)$.
	\item Def 15.2 (Distribution): The distribution of a discrete random variable $X$ is the collection of values $\{(a,\mathbb{P}[X = a]): a\in\mathscr{A}\}$, where $\mathscr{A}$ is the set of all possible values taken by $X$ (Note that the collection of events form a partition of the sample space $\Omega$).
	\item Bernoulli Distribution: $X\sim$ Bernoulli$(p)$ which takes value in $\{0,1\}$ such that $p = \mathbb{P}[X=1] = 1-\mathbb{P}[X=0]$ where $0\leq p\leq1$.
	\item Binomial Distribution: $X\sim$ Bin$(n,p)$ such that $\mathbb{P}[X=i] = \binom{n}{i}\cdot p^i(1-p)^{n-i}$ for $i=0,1,\dots,n$.
	\item If $X\sim Bin(n,p)$, then $\mathbb{E}[X] = np$. For independent $X,Y\sim Bin(n,p)$, we have $X+Y\sim Bin(2n,p)$.
	\item The Bin$(n,p)$ gives probabilistic proof for the Binomial Theorem since by properties of $X$ we have $\sum\limits_{i=0}^n \mathbb{P}[X=i] = 1 \longrightarrow \sum\limits_{i=0}^n \binom{n}{i}\cdot p^i(1-p)^{n-i} = 1$
	\item Relation to Error Correction: If we model each packet getting lost with probability $p$ and the losses are independent, then if we transmit $n+k$ packets, the number of packets received is a random variable $X$ with binomial distribution: $X\sim$ Bin$(n+k, 1-p)$, so the probability of successfully decoding the original data is: $\mathbb{P}[X\geq n] = \sum\limits_{i=n}^{n+k}\mathbb{P}[X=i] = \sum\limits_{i=n}^{n+k} \binom{n+k}{i}\cdot (1-p)^i p^{n+k-i}$
	\item Def 15.3: The joint distribution for two discrete random variables $X$ and $Y$ is the collection of values $\{((a,b),P[X=a,Y=b]):a\in\mathscr{A},b\in\mathscr{B}\}$, where $\mathscr{A}$ is the set of all possible values taken by $X$ and $\mathscr{B}$ is the set of all possible values taken by $Y$.
	\item Def 15.4 (Independence): Random variables $X$ and $Y$ on the same probability space are said to be independent if the events $X = a$ and $Y = b$ are independent for all values $a, b$. Equivalently, the joint distribution of independent r.v.’s decomposes as $\mathbb{P}[X=a,Y=b] = \mathbb{P}[X=a]\mathbb{P}[Y=b]$, $\forall a,b$.
	\item Def 15.5 (Expectation): The expectation of a discrete random variable $X$ is defined as $\mathbb{E}[X] = \sum\limits_{a\in\mathscr{A}} a\cdot\mathbb{P}[X=a]$ where the sum is over all possible values taken by the r.v. (Expectation is well defined provided that the sum on the RHS is absolutely convergent, and there are random variables for which expectations do not exist (haven't encountered yet but keep in mind)).
	\item The expectation can be seen in some sense as a ``typical'' value of the r.v. (though note that $\mathbb{E}[X]$ may not actually be a value that $X$ can take, like a regular die).
	\item Theorem 15.1 ({\color{red} Linearity of Expectation}): For any two random variables $X,Y$ on the same probability space and any constant $c$, we have $\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y], \mathbb{E}[cX] = c\mathbb{E}[X]$ regardless of whether or not $X$ and $Y$ are independent. If $X,Y$ are independent, then $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$.
	\item Indicator variable: $X_n = I_1 + \cdots + I_n$ where $I_i = 1$ if student $i$ gets their own HW and 0 otherwise. So, $\mathbb{E}$[number of students who get their own homeworks] in a class of size $n$ is 1. That is, the expected number of fixed points in a random permutation of $n$ items is always 1, regardless of $n$.
	\item HW: If $A$ and $B$ are integer-valued random variables such that for every integer $i$, $P(A = i) = P(B = i)$, then $P(A = B) > 0$ is not necessarily true. (Counterexample: Let $A$ be 0 with probability $\frac{1}{2}$ and 1 with probability $\frac{1}{2}$. Let $B = 1 - A$. Then $A$ and $B$ are never equal but $B$ takes the values 0, 1 with probability $\frac{1}{2}$ each as well.)
	\item HW: If $\mathbb{P}(A)=0$ or $\mathbb{P}(A)=1$, then $A$ is independent with itself; If $A,B$ are independent, then $\overline{A},\overline{B}$ are independent.
	\item Extra: $\mathbb{E}[X^2]\geq\mathbb{E}[X]^2$ since $\mathbb{E}[Z^2]-\mathbb{E}[Z]^2 = \mathbb{E}\big[(Z-\mathbb{E}[Z])^2\big]\geq0$ with $Z\cdot\mathbb{E}[Z] = \mathbb{E}[Z^2]$
\end{itemize}

\noindent {\large Extra Sanity Check}
\begin{itemize}
	\item For RSA, $x^{ed}\equiv x\pmod{N}$
	\item All polynomials that Lagrange Interpolation can output $\neq$ all polynomial solutions (e.g. polynomial with the coordinates $(-1,1),(0,0),(2,4)$ exist in $GF(8)$).
	\item If two degree $d$ polynomials intersect on $d + 1$ points, they must be the same polynomial.
	\item $f$ is bijection $\iff$ it is both one-to-one and onto.
	\item Prove strategy for uncountable: Diagonalization; for uncomputable: Use self-referencing algorithms OR Reduction to the Halting Problem:
	\begin{enumerate}
		\item Take input from $H$, and convert it into input for $A$.
		\item Take output from $A$, and show how it is correct and valid as output for $H$.
	\end{enumerate}
	\item There are two ways to show undecidability: Use your program as a subroutine to solve a problem we know is undecidable OR Proof by Diag..
	\item Care whether problem needs {\color{red} counting} or {\color{red} probability}.
	\item When in doubt, always calculate probability by considering the elements as distinct.
	\item Calculating probability with these steps:
	\begin{itemize}
		\item What is the sample space (i.e., the experiment and its set of possible outcomes)?
		\item What is the probability of each outcome (sample point)?
		\item What is the event we are interested in (i.e., which subset of the sample space)?
		\item Finally, compute the probability of the event by adding up the probabilities of the sample points contained in it.
	\end{itemize}
	\item When stuck on probability, consider symmetry.
	\item The independence of every pair of events (so-called pairwise independence) does not necessarily imply mutual independence (counterexample: flip first coin, flip second coin, both flips are the same).
	\item Two disjoint events A and B with $\mathbb{P}[A] > 0$ and $\mathbb{P}[B] > 0$ cannot be independent.
	\item When solving r.v. problems of intuition, always double-check the edge cases of $\mathbb{P}[A] = 0$ or 1.
	\item A random experiment must define the sample space (the set of possible outcomes) AND a set of probabilities.
	\item Combinatorial Proofs (Addition is OR, multiplication is AND)
	\begin{itemize}
		\item $\binom{n}{k+1} = \binom{n-1}{k} + \binom{n-2}{k} + \dots + \binom{k}{k}$
		\item $2^n = \sum\limits_{i=0}^n \binom{n}{i}$
		\item $(x+y)^n = \sum\limits_{k=0}^n \binom{n}{k}x^ky^{n-k}$
		\item $\binom{n}{k}\leq n^k$
		\item $\sum\limits_{i=1}^n i\binom{n}{i} = n2^{n-1}$
		\item $\sum\limits_{i=1}^n i\binom{n}{i}^2 = n\binom{2n-1}{n-1}$
		\item $\binom{2n}{n} = 2\binom{2n-1}{n-1} = \binom{2n-1}{n-1}+\binom{2n-1}{n}$
		\item $\sum\limits_{i=0}^n \binom{n}{i} \sum\limits_{j=0}^{n-i} \binom{n-i}{j} = 3^n = \sum\limits_{k=0}^{n}\binom{n}{k}2^k$
		\item $\sum_{k=b}^{a} \binom{k}{b} = \binom{a+1}{b+1}$ (16fa Q3 (4))
		\item $\sum\limits_{k=0}^n k^2\binom{n}{k} = n(n-1)2^{n-2}+n2^{n-1}$
		\item $\binom{k+n-1}{n-1} = \sum\limits_{i=0}^{k} \binom{k-i+n-2}{n-2}$
	\end{itemize}
	\item Extra: In $GF(p)$, $\exists\ p(x)$ of degree $d$ and $q(x)$ of degree $d-1$ such that a degree 1 polynomial $y(x) = p(x)$ satisfies $p(-y(0)) = 0$, where $d < p-1$ ($\equiv p,q$ share $d-1$ roots).
	\item Godel’s Incompleteness Theorem: Any formal system that is sufficiently rich to formalize arithmetic is either inconsistent (there are false statements that can be proved) or incomplete (there are true statements that cannot be proved).
\end{itemize}

\end{document}